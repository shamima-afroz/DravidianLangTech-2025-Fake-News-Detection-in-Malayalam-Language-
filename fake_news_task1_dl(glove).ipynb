{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6bfYwkQRsVs",
        "outputId": "aabf60a9-5846-4dd7-b0af-b3f7de66197c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 86 ms, sys: 13.8 ms, total: 99.8 ms\n",
            "Wall time: 227 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
        "from sklearn.metrics import average_precision_score,roc_auc_score, roc_curve, precision_recall_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "np.random.seed(42)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "import nltk, string, re, spacy,unicodedata, random\n",
        "from bs4 import BeautifulSoup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Download GloVe embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "# Convert GloVe format to Word2Vec format\n",
        "glove_input_file = 'glove.6B.300d.txt'  # Make sure the file name matches the downloaded file\n",
        "word2vec_output_file = 'word2vec.txt'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "\n",
        "# Load the converted Word2Vec model\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
        "\n",
        "# Test the model\n",
        "#print(model.most_similar('king'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBwTX4ZFbvkx",
        "outputId": "02c37f00-2808-429b-e1d6-8e7162b5fc10"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-27 17:41:43--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-01-27 17:41:43--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-01-27 17:41:43--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 38s  \n",
            "\n",
            "2025-01-27 17:44:22 (5.19 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('/content/Fake_train.csv', encoding='utf-8')\n",
        "test_df = pd.read_csv('/content/Fake_test_without_labels.csv', encoding='utf-8')\n",
        "df2 = pd.read_csv('/content/Fake_dev.csv', encoding='utf-8')\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "df = pd.read_csv('/content/Fake_train.csv')\n",
        "\n",
        "# Displaying DataFrame in Colab\n",
        "df\n",
        "\n",
        "# Preprocess data\n",
        "text_column = 'text'\n",
        "label_column = 'label'\n",
        "train_df[text_column] = train_df[text_column].fillna('')\n",
        "test_df[text_column] = test_df[text_column].fillna('')\n"
      ],
      "metadata": {
        "id": "bY6hkvTwVYMj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize the text\n",
        "max_vocab_size = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_df[text_column])\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train = tokenizer.texts_to_sequences(train_df[text_column])\n",
        "X_test = tokenizer.texts_to_sequences(test_df[text_column])\n",
        "X_dev = tokenizer.texts_to_sequences(df2[text_column])\n",
        "\n",
        "# Pad the sequences\n",
        "X_train = pad_sequences(X_train, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "X_dev = pad_sequences(X_dev, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Print a sample\n",
        "print(X_train[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACjWYejYVYZv",
        "outputId": "4550455c-96c3-4ffd-fa99-a1a0d7bf98f5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  38  328  111  329  991    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [4551 1629    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [4552 1229 1630    6    6 2398 4553 2399   73 4554  216 1230   10  295\n",
            "  4555 1631    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [4556 1632  992 4557 4558    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [4559 4560 4561  993  186 4562 4563    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_dim = 300  # This should match the dimension of the GloVe file used\n",
        "num_words = min(max_vocab_size, len(tokenizer.word_index) + 1)\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < num_words:\n",
        "        embedding_vector = model[word] if word in model else None\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNFbmxMTkiM5",
        "outputId": "665a39ab-e851-43f2-ec2a-e7fd53e53dd9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix shape: (10000, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_df[label_column])\n",
        "y_dev = label_encoder.transform(df2[label_column])\n",
        "\n",
        "# Check the encoded labels\n",
        "print(y_train[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk3oJlEUkiWj",
        "outputId": "25e10cd0-a301-46e2-a6d0-0306491dc8ad"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#lstm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=num_words,\n",
        "              output_dim=embedding_dim,\n",
        "              weights=[embedding_matrix],\n",
        "              input_length=max_sequence_length,\n",
        "              trainable=False),  # Freeze the GloVe embeddings\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_dev, y_dev),\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "#lstm\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = model.evaluate(X_dev, y_dev)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# Predict on the test data\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to binary labels\n",
        "test_labels = (test_predictions > 0.5).astype(int).flatten()\n",
        "test_df['predicted_label'] = test_labels\n",
        "\n",
        "# Save predictions\n",
        "test_df.to_csv('/content/test_predictions.csv', index=False)\n",
        "print(\"Predictions saved to /content/test_predictions.csv\")\n",
        "#lstm\n",
        "# Evaluate the model on the dev set\n",
        "val_loss, val_accuracy = model.evaluate(X_dev, y_dev)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_dev_pred = model.predict(X_dev)\n",
        "y_dev_labels = (y_dev_pred > 0.5).astype(int)\n",
        "print(classification_report(y_dev, y_dev_labels))\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the test dataset\n",
        "test_data = pd.read_csv('/content/Fake_test_without_labels.csv')\n",
        "\n",
        "# Assuming the text column in the test dataset is named 'text'\n",
        "test_texts = test_data['text'].values\n",
        "\n",
        "# Tokenize and pad the test texts (ensure you use the same tokenizer as training)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Predict labels for the test set\n",
        "test_predictions = model.predict(test_padded)\n",
        "\n",
        "# Convert predictions to binary labels (threshold = 0.5)\n",
        "test_labels = (test_predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "# Add predictions to the test dataframe\n",
        "test_data['predicted_label'] = test_labels\n",
        "\n",
        "# Save the results to a new CSV file with the model name in the filename\n",
        "output_path = '/content/lstm_predicted_test.csv'\n",
        "test_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_path}\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the predictions dataset\n",
        "predictions_data = pd.read_csv('/content/lstm_predicted_test.csv')\n",
        "\n",
        "# Map numeric predictions (0 and 1) to descriptive labels\n",
        "label_mapping = {0: 'Fake', 1: 'original'}\n",
        "predictions_data['predicted_label'] = predictions_data['predicted_label'].map(label_mapping)\n",
        "\n",
        "# Keep only the 'id' and 'predicted_label' columns\n",
        "output_data = predictions_data[['Id', 'predicted_label']]\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "output_path = '/content/lstm_cleaned_test.csv'\n",
        "output_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Cleaned predictions saved to {output_path}\")\n",
        "\n",
        "#lstm\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the files\n",
        "predicted_file = pd.read_csv('/content/lstm_cleaned_test.csv')\n",
        "actual_file = pd.read_csv('/content/Fake_test_with_labels.csv')\n",
        "\n",
        "# Verify column names\n",
        "print(\"Predicted file columns:\", predicted_file.columns)\n",
        "print(\"Actual file columns:\", actual_file.columns)\n",
        "\n",
        "# Create the 'Id' column in the actual file\n",
        "# Assuming the row order corresponds to IDs like Fake_01, Fake_02\n",
        "actual_file['Id'] = ['Fake_{:02d}'.format(i + 1) for i in range(len(actual_file))]\n",
        "\n",
        "# Verify the 'Id' column\n",
        "print(\"Actual file with 'Id' column:\")\n",
        "print(actual_file.head())\n",
        "\n",
        "# Merge the files on the 'Id' column\n",
        "merged = pd.merge(predicted_file, actual_file, on='Id')\n",
        "\n",
        "# Extract the true labels and predicted labels\n",
        "y_true = merged['label']  # Actual labels\n",
        "y_pred = merged['predicted_label']  # Predicted labels\n",
        "\n",
        "# Compare predictions and compute metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Save the merged file with comparison for review (optional)\n",
        "merged.to_csv('/content/lstm_merged_comparison.csv', index=False)\n",
        "print(\"Merged comparison file saved to /content/lstm_merged_comparison.csv\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "m8HtjcFOkiZ9",
        "outputId": "f5e78e9a-8e6c-497a-b99d-2a7c977ba7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │       \u001b[38;5;34m3,000,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,000,000\u001b[0m (11.44 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> (11.44 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,000,000\u001b[0m (11.44 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> (11.44 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 193ms/step - accuracy: 0.4913 - loss: 0.6926 - val_accuracy: 0.5252 - val_loss: 0.6890\n",
            "Epoch 2/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 164ms/step - accuracy: 0.5432 - loss: 0.6909 - val_accuracy: 0.5104 - val_loss: 0.6912\n",
            "Epoch 3/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 138ms/step - accuracy: 0.5854 - loss: 0.6810 - val_accuracy: 0.6025 - val_loss: 0.6700\n",
            "Epoch 4/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 191ms/step - accuracy: 0.6127 - loss: 0.6693 - val_accuracy: 0.5067 - val_loss: 0.6983\n",
            "Epoch 5/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 150ms/step - accuracy: 0.5091 - loss: 0.6950 - val_accuracy: 0.5067 - val_loss: 0.6921\n",
            "Epoch 6/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 144ms/step - accuracy: 0.5108 - loss: 0.6913 - val_accuracy: 0.5067 - val_loss: 0.6914\n",
            "Epoch 7/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 197ms/step - accuracy: 0.5023 - loss: 0.6924 - val_accuracy: 0.5423 - val_loss: 0.6838\n",
            "Epoch 8/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 145ms/step - accuracy: 0.5844 - loss: 0.6753 - val_accuracy: 0.6000 - val_loss: 0.6705\n",
            "Epoch 9/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 150ms/step - accuracy: 0.5683 - loss: 0.6771 - val_accuracy: 0.5067 - val_loss: 0.6928\n",
            "Epoch 10/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 342ms/step - accuracy: 0.5173 - loss: 0.6927 - val_accuracy: 0.5067 - val_loss: 0.6920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lstm+bilstm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "\n",
        "# Build the model with LSTM + BiLSTM\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=num_words,\n",
        "              output_dim=embedding_dim,\n",
        "              weights=[embedding_matrix],\n",
        "              input_length=max_sequence_length,\n",
        "              trainable=False),  # Freeze the GloVe embeddings\n",
        "    LSTM(64, return_sequences=True),  # First LSTM layer with return_sequences=True for BiLSTM\n",
        "    Bidirectional(LSTM(64, return_sequences=False)),  # BiLSTM layer\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary to see the architecture\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_dev, y_dev),\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on the dev set\n",
        "val_loss, val_accuracy = model.evaluate(X_dev, y_dev)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# Classification report\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_dev_pred = model.predict(X_dev)\n",
        "y_dev_labels = (y_dev_pred > 0.5).astype(int)\n",
        "print(classification_report(y_dev, y_dev_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "id": "Q_wI5CU_kiqd",
        "outputId": "3cff3d5e-7151-4a75-b811-5bd9d6baa641"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (\u001b[38;5;33mEmbedding\u001b[0m)             │ ?                           │       \u001b[38;5;34m3,000,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_14 (\u001b[38;5;33mLSTM\u001b[0m)                       │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_7 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)                 │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,000,000\u001b[0m (11.44 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> (11.44 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,000,000\u001b[0m (11.44 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> (11.44 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 327ms/step - accuracy: 0.5833 - loss: 0.6679 - val_accuracy: 0.6368 - val_loss: 0.6201\n",
            "Epoch 2/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 330ms/step - accuracy: 0.6499 - loss: 0.6139 - val_accuracy: 0.6515 - val_loss: 0.6028\n",
            "Epoch 3/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 328ms/step - accuracy: 0.6710 - loss: 0.5893 - val_accuracy: 0.6528 - val_loss: 0.6030\n",
            "Epoch 4/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 308ms/step - accuracy: 0.6702 - loss: 0.5637 - val_accuracy: 0.6466 - val_loss: 0.6044\n",
            "Epoch 5/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 322ms/step - accuracy: 0.6768 - loss: 0.5523 - val_accuracy: 0.6331 - val_loss: 0.6504\n",
            "Epoch 6/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 325ms/step - accuracy: 0.6849 - loss: 0.5519 - val_accuracy: 0.6393 - val_loss: 0.6112\n",
            "Epoch 7/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 420ms/step - accuracy: 0.6945 - loss: 0.5276 - val_accuracy: 0.6405 - val_loss: 0.6243\n",
            "Epoch 8/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 321ms/step - accuracy: 0.6969 - loss: 0.5093 - val_accuracy: 0.6466 - val_loss: 0.6436\n",
            "Epoch 9/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 331ms/step - accuracy: 0.7100 - loss: 0.5058 - val_accuracy: 0.6564 - val_loss: 0.6501\n",
            "Epoch 10/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 330ms/step - accuracy: 0.7199 - loss: 0.4859 - val_accuracy: 0.6552 - val_loss: 0.6523\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.6545 - loss: 0.6369\n",
            "Validation Loss: 0.6522820591926575, Validation Accuracy: 0.6552147269248962\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.41      0.54       406\n",
            "           1       0.60      0.90      0.72       409\n",
            "\n",
            "    accuracy                           0.66       815\n",
            "   macro avg       0.70      0.65      0.63       815\n",
            "weighted avg       0.70      0.66      0.63       815\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Make Predictions on the Dev Set\n",
        "y_dev_pred = model.predict(X_dev)\n",
        "y_dev_labels = (y_dev_pred > 0.5).astype(int)  # Convert probabilities to binary labels (0 or 1)\n",
        "\n",
        "# 2. Classification Report on Dev Set\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report on Dev Set:\")\n",
        "print(classification_report(y_dev, y_dev_labels))\n",
        "\n",
        "# 3. Make Predictions on the Test Set\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the test dataset\n",
        "test_data = pd.read_csv('/content/Fake_test_without_labels.csv')  # Replace with actual test data path\n",
        "\n",
        "# Assuming the text column in the test dataset is named 'text'\n",
        "test_texts = test_data['text'].values\n",
        "\n",
        "# Tokenize and pad the test texts (ensure you use the same tokenizer as during training)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Predict labels for the test set\n",
        "test_predictions = model.predict(test_padded)\n",
        "\n",
        "# Convert predictions to binary labels (threshold = 0.5)\n",
        "test_labels = (test_predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "# 4. Add Predictions to the Test Dataframe\n",
        "test_data['predicted_label'] = test_labels\n",
        "\n",
        "# Save the results to a new CSV file with the model name in the filename\n",
        "output_path = '/content/lstm_bilstm_predicted_test.csv'\n",
        "test_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_path}\")\n",
        "\n",
        "# 5. Map numeric predictions (0 and 1) to descriptive labels (Fake/Original)\n",
        "label_mapping = {0: 'Fake', 1: 'original'}\n",
        "test_data['predicted_label'] = test_data['predicted_label'].map(label_mapping)\n",
        "\n",
        "# Keep only the 'Id' and 'predicted_label' columns\n",
        "output_data = test_data[['Id', 'predicted_label']]\n",
        "\n",
        "# Save the cleaned predictions to a new CSV file\n",
        "output_path_cleaned = '/content/lstm_bilstm_cleaned_test.csv'\n",
        "output_data.to_csv(output_path_cleaned, index=False)\n",
        "\n",
        "print(f\"Cleaned predictions saved to {output_path_cleaned}\")\n",
        "\n",
        "# 6. Evaluate the Model on the Test Set\n",
        "# Assuming you have the actual labels for the test set in a separate file\n",
        "actual_file = pd.read_csv('/content/Fake_test_with_labels.csv')  # Replace with actual test labels file\n",
        "\n",
        "# Verify column names and create 'Id' column in the actual file\n",
        "actual_file['Id'] = ['Fake_{:02d}'.format(i + 1) for i in range(len(actual_file))]\n",
        "\n",
        "# Merge the predicted and actual labels on the 'Id' column\n",
        "merged = pd.merge(output_data, actual_file, on='Id')\n",
        "\n",
        "# Extract the true labels and predicted labels\n",
        "y_true = merged['label']  # Actual labels\n",
        "y_pred = merged['predicted_label']  # Predicted labels\n",
        "\n",
        "# Compare predictions and compute metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"Accuracy on Test Set:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Classification Report on Test Set:\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Save the merged file with comparison for review (optional)\n",
        "merged.to_csv('/content/lstm_bilstm_merged_comparison.csv', index=False)\n",
        "print(\"Merged comparison file saved to /content/lstm_bilstm_merged_comparison.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry5XB-fSkitq",
        "outputId": "b97b02bd-6c6d-4dfa-e4af-8134c7868cc3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step\n",
            "Classification Report on Dev Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.41      0.54       406\n",
            "           1       0.60      0.90      0.72       409\n",
            "\n",
            "    accuracy                           0.66       815\n",
            "   macro avg       0.70      0.65      0.63       815\n",
            "weighted avg       0.70      0.66      0.63       815\n",
            "\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 133ms/step\n",
            "Predictions saved to /content/lstm_bilstm_predicted_test.csv\n",
            "Cleaned predictions saved to /content/lstm_bilstm_cleaned_test.csv\n",
            "Accuracy on Test Set: 0.6545632973503435\n",
            "Classification Report on Test Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.80      0.40      0.54       507\n",
            "    original       0.60      0.90      0.72       512\n",
            "\n",
            "    accuracy                           0.65      1019\n",
            "   macro avg       0.70      0.65      0.63      1019\n",
            "weighted avg       0.70      0.65      0.63      1019\n",
            "\n",
            "Merged comparison file saved to /content/lstm_bilstm_merged_comparison.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bilstm\n",
        "# Import necessary modules\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Define the BiLSTM model\n",
        "model_bilstm = Sequential([\n",
        "    # Embedding layer with pre-trained GloVe embeddings\n",
        "    Embedding(input_dim=num_words,\n",
        "              output_dim=embedding_dim,\n",
        "              weights=[embedding_matrix],\n",
        "              input_length=max_sequence_length,\n",
        "              trainable=False),  # Freeze the GloVe embeddings\n",
        "\n",
        "    # BiLSTM layer - Bidirectional LSTM\n",
        "    Bidirectional(LSTM(64, return_sequences=False)),  # Set return_sequences=False for classification tasks\n",
        "\n",
        "    # Dropout layer to prevent overfitting\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Fully connected dense layer with L2 regularization\n",
        "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Output layer for binary classification (sigmoid activation)\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_bilstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary to see the layers and parameters\n",
        "model_bilstm.summary()\n",
        "\n",
        "# Train the model\n",
        "history_bilstm = model_bilstm.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_dev, y_dev),\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on the dev set\n",
        "val_loss, val_accuracy = model_bilstm.evaluate(X_dev, y_dev)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# Classification report for the dev set\n",
        "from sklearn.metrics import classification_report\n",
        "y_dev_pred = model_bilstm.predict(X_dev)\n",
        "y_dev_labels = (y_dev_pred > 0.5).astype(int)\n",
        "print(\"Classification Report on Dev Set:\")\n",
        "print(classification_report(y_dev, y_dev_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        },
        "id": "99_ZcJixkiwt",
        "outputId": "d5cc7366-d496-46a6-f196-62f2951553eb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │       \u001b[38;5;34m3,000,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_6 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)                 │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)                 │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,000,000\u001b[0m (11.44 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> (11.44 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,000,000\u001b[0m (11.44 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> (11.44 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 347ms/step - accuracy: 0.5782 - loss: 1.5091 - val_accuracy: 0.6282 - val_loss: 0.7887\n",
            "Epoch 2/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 435ms/step - accuracy: 0.6380 - loss: 0.7417 - val_accuracy: 0.6344 - val_loss: 0.6408\n",
            "Epoch 3/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 339ms/step - accuracy: 0.6663 - loss: 0.6198 - val_accuracy: 0.4994 - val_loss: 1.3296\n",
            "Epoch 4/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 338ms/step - accuracy: 0.6524 - loss: 0.6415 - val_accuracy: 0.6393 - val_loss: 0.6176\n",
            "Epoch 5/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 334ms/step - accuracy: 0.6777 - loss: 0.5848 - val_accuracy: 0.6405 - val_loss: 0.6190\n",
            "Epoch 6/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 342ms/step - accuracy: 0.7003 - loss: 0.5623 - val_accuracy: 0.6356 - val_loss: 0.6342\n",
            "Epoch 7/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 339ms/step - accuracy: 0.7014 - loss: 0.5536 - val_accuracy: 0.6405 - val_loss: 0.6322\n",
            "Epoch 8/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 346ms/step - accuracy: 0.6998 - loss: 0.5613 - val_accuracy: 0.5006 - val_loss: 0.7035\n",
            "Epoch 9/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 409ms/step - accuracy: 0.5331 - loss: 0.6705 - val_accuracy: 0.6442 - val_loss: 0.6688\n",
            "Epoch 10/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 327ms/step - accuracy: 0.6800 - loss: 0.6317 - val_accuracy: 0.6528 - val_loss: 0.6260\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.6511 - loss: 0.6376\n",
            "Validation Loss: 0.6260471343994141, Validation Accuracy: 0.6527607440948486\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step\n",
            "Classification Report on Dev Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.41      0.54       406\n",
            "           1       0.60      0.90      0.72       409\n",
            "\n",
            "    accuracy                           0.65       815\n",
            "   macro avg       0.70      0.65      0.63       815\n",
            "weighted avg       0.70      0.65      0.63       815\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bilstm\n",
        "# Step 1: Evaluate the model on the dev set (already done above)\n",
        "val_loss, val_accuracy = model_bilstm.evaluate(X_dev, y_dev)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# Step 2: Generate predictions on the dev set\n",
        "y_dev_pred = model_bilstm.predict(X_dev)\n",
        "\n",
        "# Convert the predictions to binary labels (threshold = 0.5)\n",
        "y_dev_labels = (y_dev_pred > 0.5).astype(int)\n",
        "\n",
        "# Print the classification report for the dev set\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report on Dev Set:\")\n",
        "print(classification_report(y_dev, y_dev_labels))\n",
        "\n",
        "# Step 3: Predict on the test set\n",
        "# Load the test data (make sure you use the same tokenizer and padding as training)\n",
        "test_data = pd.read_csv('/content/Fake_test_without_labels.csv')\n",
        "\n",
        "# Assuming the test data has a 'text' column\n",
        "test_texts = test_data['text'].values\n",
        "\n",
        "# Tokenize and pad the test texts (ensure you use the same tokenizer as training)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Predict the labels for the test set\n",
        "test_predictions = model_bilstm.predict(test_padded)\n",
        "\n",
        "# Convert predictions to binary labels (threshold = 0.5)\n",
        "test_labels = (test_predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "# Add the predicted labels to the test dataframe\n",
        "test_data['predicted_label'] = test_labels\n",
        "\n",
        "# Step 4: Save the test predictions to a CSV file\n",
        "output_path = '/content/bilstm_predicted_test.csv'\n",
        "test_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_path}\")\n",
        "\n",
        "# Step 5: Clean the predictions (mapping numeric values to human-readable labels)\n",
        "label_mapping = {0: 'Fake', 1: 'original'}\n",
        "test_data['predicted_label'] = test_data['predicted_label'].map(label_mapping)\n",
        "\n",
        "# Keep only the 'Id' and 'predicted_label' columns for final submission\n",
        "output_data = test_data[['Id', 'predicted_label']]\n",
        "\n",
        "# Step 6: Save the cleaned predictions to a new CSV file\n",
        "final_output_path = '/content/bilstm_cleaned_test.csv'\n",
        "output_data.to_csv(final_output_path, index=False)\n",
        "\n",
        "print(f\"Cleaned predictions saved to {final_output_path}\")\n",
        "\n",
        "# Step 7: Evaluate the predictions with the actual labels (if available)\n",
        "# Load the actual test labels (assuming they are available in another file)\n",
        "actual_file = pd.read_csv('/content/Fake_test_with_labels.csv')\n",
        "\n",
        "# Add the 'Id' column to match with the predictions file\n",
        "actual_file['Id'] = ['Fake_{:02d}'.format(i + 1) for i in range(len(actual_file))]\n",
        "\n",
        "# Merge predictions and actual labels on 'Id'\n",
        "merged = pd.merge(output_data, actual_file, on='Id')\n",
        "\n",
        "# Extract the true and predicted labels for evaluation\n",
        "y_true = merged['label']  # Actual labels\n",
        "y_pred = merged['predicted_label']  # Predicted labels\n",
        "\n",
        "# Compare predictions and compute metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Optional: Save the merged file with comparison (if needed)\n",
        "comparison_output_path = '/content/bilstm_merged_comparison.csv'\n",
        "merged.to_csv(comparison_output_path, index=False)\n",
        "print(f\"Merged comparison file saved to {comparison_output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkyzP-mCki0B",
        "outputId": "4b7efe15-2a17-494f-83ad-b658191b0cf9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 202ms/step - accuracy: 0.6511 - loss: 0.6376\n",
            "Validation Loss: 0.6260471343994141, Validation Accuracy: 0.6527607440948486\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step\n",
            "Classification Report on Dev Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.41      0.54       406\n",
            "           1       0.60      0.90      0.72       409\n",
            "\n",
            "    accuracy                           0.65       815\n",
            "   macro avg       0.70      0.65      0.63       815\n",
            "weighted avg       0.70      0.65      0.63       815\n",
            "\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step\n",
            "Predictions saved to /content/bilstm_predicted_test.csv\n",
            "Cleaned predictions saved to /content/bilstm_cleaned_test.csv\n",
            "Accuracy: 0.6526005888125613\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.82      0.39      0.53       507\n",
            "    original       0.60      0.91      0.73       512\n",
            "\n",
            "    accuracy                           0.65      1019\n",
            "   macro avg       0.71      0.65      0.63      1019\n",
            "weighted avg       0.71      0.65      0.63      1019\n",
            "\n",
            "Merged comparison file saved to /content/bilstm_merged_comparison.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "927JuwfUki26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8QKBcYWaki7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJxEKopski-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_is3MUQjVYtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jimiL-oDVYxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iDsNnCixVY2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cwEudbTXVY5g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}