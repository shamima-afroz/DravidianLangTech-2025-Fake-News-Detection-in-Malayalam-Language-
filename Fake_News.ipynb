{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10518353,"sourceType":"datasetVersion","datasetId":6266894}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T07:45:29.436775Z","iopub.execute_input":"2025-01-26T07:45:29.437140Z","iopub.status.idle":"2025-01-26T07:45:30.508137Z","shell.execute_reply.started":"2025-01-26T07:45:29.437110Z","shell.execute_reply":"2025-01-26T07:45:30.507107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\ntrain= pd.read_csv('/kaggle/input/fake-news/Fake_train.csv')\ndev= pd.read_csv('/kaggle/input/fake-news/Fake_dev.csv')\ntest=pd.read_csv(\"/kaggle/input/fake-news/Fake_test_without_labels.csv\")\ntest_label= pd.read_csv('/kaggle/input/fake-news/Fake_test_with_labels.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T07:46:33.132657Z","iopub.execute_input":"2025-01-26T07:46:33.133701Z","iopub.status.idle":"2025-01-26T07:46:33.228909Z","shell.execute_reply.started":"2025-01-26T07:46:33.133655Z","shell.execute_reply":"2025-01-26T07:46:33.228068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T19:18:02.951591Z","iopub.execute_input":"2025-01-19T19:18:02.951990Z","iopub.status.idle":"2025-01-19T19:18:02.964975Z","shell.execute_reply.started":"2025-01-19T19:18:02.951958Z","shell.execute_reply":"2025-01-19T19:18:02.963439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_label['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T19:18:12.097868Z","iopub.execute_input":"2025-01-19T19:18:12.098244Z","iopub.status.idle":"2025-01-19T19:18:12.114079Z","shell.execute_reply.started":"2025-01-19T19:18:12.098208Z","shell.execute_reply":"2025-01-19T19:18:12.112945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine all datasets into one DataFrame\ncombined_data = pd.concat([train, dev, test_label], ignore_index=True)\n\n# Ensure the class column exists and is consistent\n# Replace 'class_column_name' with the actual name of the column indicating 'original' or 'fake' class.\n# Replace 'text_column_name' with the actual name of the column containing text.\n\nclass_column = 'label'  # Change to the correct column name\ntext_column = 'text'    # Change to the correct column name\n\n# Filter rows by class and compute total word count\ndef count_words_by_class(data, label):\n    # Filter rows by the class label\n    class_data = data[data[class_column] == label]\n    \n    # Combine all text for the class into a single string and count words\n    total_words = class_data[text_column].str.split().str.len().sum()\n    return total_words\n\n# Count words for each class\ntotal_original_words = count_words_by_class(combined_data, 'original')\ntotal_fake_words = count_words_by_class(combined_data, 'Fake')\ntotal_words = total_original_words + total_fake_words\n# Print the results\nprint(f\"Total words in 'original' class: {total_original_words}\")\nprint(f\"Total words in 'fake' class: {total_fake_words}\")\nprint(f\"Total words in the dataset: {total_words}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T19:26:29.443663Z","iopub.execute_input":"2025-01-19T19:26:29.444034Z","iopub.status.idle":"2025-01-19T19:26:29.478570Z","shell.execute_reply.started":"2025-01-19T19:26:29.444001Z","shell.execute_reply":"2025-01-19T19:26:29.477162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T07:46:37.246162Z","iopub.execute_input":"2025-01-26T07:46:37.246993Z","iopub.status.idle":"2025-01-26T07:46:55.040624Z","shell.execute_reply.started":"2025-01-26T07:46:37.246946Z","shell.execute_reply":"2025-01-26T07:46:55.039851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count of duplicate texts\ntrain['text'].duplicated().sum()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T04:25:50.886317Z","iopub.execute_input":"2024-12-16T04:25:50.886664Z","iopub.status.idle":"2024-12-16T04:25:50.893549Z","shell.execute_reply.started":"2024-12-16T04:25:50.886634Z","shell.execute_reply":"2024-12-16T04:25:50.892749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ai4bharat-transliteration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T07:46:55.042074Z","iopub.execute_input":"2025-01-26T07:46:55.042553Z","iopub.status.idle":"2025-01-26T07:47:54.733170Z","shell.execute_reply.started":"2025-01-26T07:46:55.042524Z","shell.execute_reply":"2025-01-26T07:47:54.732088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom bs4 import BeautifulSoup\nfrom ai4bharat.transliteration import XlitEngine\nfrom tqdm import tqdm\n!pip install malaya\n\nfrom malaya.text.function import STOPWORDS\n\nstop_words = STOPWORDS\n# Ensure necessary resources are downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\n\ne = XlitEngine(\"ml\", beam_width=10)\n\n\n# Define punctuations to remove\npunctuations = '''’'!()-[]{};:'\"\\,<>./?@#$%^&*_~�'''\n\n# Function to remove URLs and punctuations\ndef remove_punctuation_url(text):\n    text = text.lower()\n    # Remove URLs\n    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n    # Remove unwanted characters\n    text = text.replace('\\n', ' ')\n    # Remove punctuations\n    text = ''.join(char for char in text if char not in punctuations)\n    return text\n\n# Function to remove stopwords\ndef remove_stopwords(text):\n    tokens = word_tokenize(text.lower())\n    tokens_without_sw = [word for word in tokens if word not in stop_words]\n    return ' '.join(tokens_without_sw)\n\n# Function to remove HTML tags\ndef remove_html_tags(text):\n    return BeautifulSoup(text, 'html.parser').get_text()\n\n# Function to transliterate text into Malayalam\ndef transliterate_to_malayalam(text):\n    result = e.translit_sentence(text)\n    return result[\"ml\"]\ndef remove_stopwords(text):\n    words = text.split()\n    filtered_words = [word for word in words if word not in stop_words]\n    return ' '.join(filtered_words)\n\n# Master function for preprocessing\ndef full_preprocessing(text):\n    text = remove_html_tags(text)  \n    text = remove_punctuation_url(text)  # Remove URLs and punctuations\n    text = remove_stopwords(text)  # Remove stopwords\n    text = transliterate_to_malayalam(text)  # Transliterate to Malayalam\n    return text\n\n# Apply preprocessing on training, validation, and test datasets\ntqdm.pandas(desc=\"Processing Text\")\n\ntrain['cleaned'] = train['text'].progress_apply(full_preprocessing)\ndev['cleaned'] = dev['text'].progress_apply(full_preprocessing)\ntest['cleaned'] = test['text'].progress_apply(full_preprocessing)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T07:47:54.734576Z","iopub.execute_input":"2025-01-26T07:47:54.734898Z","iopub.status.idle":"2025-01-26T08:17:40.106842Z","shell.execute_reply.started":"2025-01-26T07:47:54.734869Z","shell.execute_reply":"2025-01-26T08:17:40.105907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n#original:1 and Fake:0\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Fit and transform the labels\ntrain['label'] = encoder.fit_transform(train['label'])\ndev['label'] = encoder.transform(dev['label'])\n\n# Verify the conversion\nprint(train['label'].value_counts())\nprint(dev['label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T08:19:54.775917Z","iopub.execute_input":"2025-01-26T08:19:54.776289Z","iopub.status.idle":"2025-01-26T08:19:54.796252Z","shell.execute_reply.started":"2025-01-26T08:19:54.776260Z","shell.execute_reply":"2025-01-26T08:19:54.795232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T17:00:14.172043Z","iopub.execute_input":"2025-01-09T17:00:14.172400Z","iopub.status.idle":"2025-01-09T17:00:14.201203Z","shell.execute_reply.started":"2025-01-09T17:00:14.172368Z","shell.execute_reply":"2025-01-09T17:00:14.200342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"google/muril-base-cased\", num_labels=2)\n\n# Tokenize datasets\nmax_length = 60\ntrain_encodings = tokenizer(list(train['text']), truncation=True, padding=True, max_length=max_length)\ndev_encodings = tokenizer(list(dev['text']), truncation=True, padding=True, max_length=max_length)\n\n# Convert to TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    train['label']\n)).shuffle(len(train)).batch(16)\n\ndev_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(dev_encodings),\n    dev['label']\n)).batch(16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:45:31.077506Z","iopub.execute_input":"2025-01-25T18:45:31.077877Z","iopub.status.idle":"2025-01-25T18:45:44.749282Z","shell.execute_reply.started":"2025-01-25T18:45:31.077844Z","shell.execute_reply":"2025-01-25T18:45:44.748337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFAutoModelForSequenceClassification, AutoTokenizer\nfrom sklearn.preprocessing import LabelEncoder\n\nmodel_name = \"google/muril-base-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T04:26:21.477174Z","iopub.status.idle":"2024-12-16T04:26:21.477511Z","shell.execute_reply.started":"2024-12-16T04:26:21.477349Z","shell.execute_reply":"2024-12-16T04:26:21.477367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_texts(texts, tokenizer, max_length=60):\n    return tokenizer(\n        texts,\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"tf\"\n    )\n\ntrain_texts = tokenize_texts(train['text'].tolist(), tokenizer)\ndev_texts = tokenize_texts(dev['text'].tolist(), tokenizer)\n\n# Convert labels to tensors\ntrain_labels = tf.convert_to_tensor(train['label'].tolist())\ndev_labels = tf.convert_to_tensor(dev['label'].tolist())\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\ntrain_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\nval_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n\n# Prepare data\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": train_texts[\"input_ids\"], \"attention_mask\": train_texts[\"attention_mask\"]},\n    train_labels\n)).batch(16)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": dev_texts[\"input_ids\"], \"attention_mask\": dev_texts[\"attention_mask\"]},\n    dev_labels\n)).batch(16)\n\n# Custom training loop\nepochs = 12\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n    \n    # Training\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n        with tf.GradientTape() as tape:\n            logits = model(x_batch_train, training=True).logits\n            loss = loss_fn(y_batch_train, logits)\n        \n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        train_acc_metric.update_state(y_batch_train, logits)\n\n    train_acc = train_acc_metric.result()\n    print(f\"Training accuracy: {float(train_acc):.4f}\")\n    train_acc_metric.reset_state()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T18:45:48.178740Z","iopub.execute_input":"2025-01-25T18:45:48.179164Z","iopub.status.idle":"2025-01-25T19:13:57.879392Z","shell.execute_reply.started":"2025-01-25T18:45:48.179128Z","shell.execute_reply":"2025-01-25T19:13:57.878448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, f1_score\n\n# Separate labels from the dev dataset\ndev_texts = dev['text'].tolist()  # Texts for prediction\ndev_labels = dev['label'].tolist()  # True labels\n\n# Tokenize dev texts\ndev_inputs = tokenize_texts(dev_texts, tokenizer)\n\n# Predict on dev texts\ndev_logits = model.predict({\"input_ids\": dev_inputs[\"input_ids\"], \n                            \"attention_mask\": dev_inputs[\"attention_mask\"]}).logits\n\n# Convert logits to predicted class labels\npredicted_labels = tf.argmax(dev_logits, axis=1).numpy()\n\n# Evaluate model performance\naccuracy = accuracy_score(dev_labels, predicted_labels)\nf1 = f1_score(dev_labels, predicted_labels, average=\"weighted\")\nreport = classification_report(dev_labels, predicted_labels, target_names=[\"Class 0\", \"Class 1\"])\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:23:22.504992Z","iopub.execute_input":"2025-01-25T19:23:22.505698Z","iopub.status.idle":"2025-01-25T19:23:26.377836Z","shell.execute_reply.started":"2025-01-25T19:23:22.505666Z","shell.execute_reply":"2025-01-25T19:23:26.376975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nimport pandas as pd\n\n# Step 1: Ensure the correct labels and predicted labels are available\ntest['correct_label'] = test_label['label']  # Add the true labels to the test DataFrame\n\n# Step 2: Evaluate the model's performance\naccuracy = accuracy_score(test['correct_label'], test['label'])\nprecision = precision_score(test['correct_label'], test['label'], average='weighted')\nrecall = recall_score(test['correct_label'], test['label'], average='weighted')\nf1 = f1_score(test['correct_label'], test['label'], average='weighted')\nreport = classification_report(test['correct_label'], test['label'], target_names=[\"Fake\", \"original\"])\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(\"\\nClassification Report:\\n\", report)\n\n# Step 3: Create a comparison DataFrame for the first 5 rows\ncomparison_df = test[['text', 'correct_label', 'label']].head(5).rename(\n    columns={'text': 'News', 'correct_label': 'True Label', 'label': 'Predicted Label'}\n)\n\n# Step 4: Display the comparison\nprint(\"\\nComparison of the first 5 rows:\")\nprint(comparison_df)\n\n# Optionally, you can save the comparison DataFrame for reference\ncomparison_df.to_csv('comparison_first_5.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:27:19.348148Z","iopub.execute_input":"2025-01-25T19:27:19.348603Z","iopub.status.idle":"2025-01-25T19:27:19.441494Z","shell.execute_reply.started":"2025-01-25T19:27:19.348561Z","shell.execute_reply":"2025-01-25T19:27:19.440565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nSample Predictions vs Actual Labels:\")\nfor i in range(50):  \n    print(f\"Predicted: {predicted_labels[i]}, Actual: {dev_labels[i]}\")\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T15:44:05.172960Z","iopub.execute_input":"2024-12-14T15:44:05.173334Z","iopub.status.idle":"2024-12-14T15:44:05.179539Z","shell.execute_reply.started":"2024-12-14T15:44:05.173304Z","shell.execute_reply":"2024-12-14T15:44:05.178582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\n\n# Step 1: Get the test texts\ntest_texts = test['text'].tolist()\ntest_inputs = tokenize_texts(test_texts, tokenizer)\n\n# Step 2: Predict labels for test data\ntest_logits = model.predict({\n    \"input_ids\": test_inputs[\"input_ids\"], \n    \"attention_mask\": test_inputs[\"attention_mask\"]\n}).logits\n\n# Convert logits to predicted class labels (numerical)\ntest['label'] = tf.argmax(test_logits, axis=1).numpy()\n\n# Step 3: Map numerical labels back to \"fake\" and \"original\"\nlabel_mapping = {0: \"Fake\", 1: \"original\"}\ntest['label'] = test['label'].map(label_mapping)\n\n# Step 4: Create submission CSV files for multiple runs\nteam_name = \"One_by_zero\"\nsubmission_dir = f\"{team_name}\"\nos.makedirs(submission_dir, exist_ok=True)\n\nsubmission_file = f\"{submission_dir}/{team_name}_Malayalam_Task1_run1.csv\"\ntest[['Id', 'label']].rename(columns={'label': 'Labels'}).to_csv(submission_file, index=False)\n\n# Step 5: Zip the submission files\nzip_filename = f\"{team_name}.zip\"\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    submission_file = f\"{submission_dir}/{team_name}_Malayalam_Task1_run1.csv\"\n    zipf.write(submission_file)\n\nprint(f\"Submission file {zip_filename} created successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:27:08.923889Z","iopub.execute_input":"2025-01-25T19:27:08.924241Z","iopub.status.idle":"2025-01-25T19:27:14.473824Z","shell.execute_reply.started":"2025-01-25T19:27:08.924202Z","shell.execute_reply":"2025-01-25T19:27:14.472984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n\nmodel_name = \"eliasedwin7/MalayalamBERT\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load the PyTorch model into TensorFlow\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, from_pt=True)\n\ndef tokenize_texts(texts, tokenizer, max_length=128):\n    return tokenizer(\n        texts,\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"tf\"\n    )\n\ntrain_texts = tokenize_texts(train['text'].tolist(), tokenizer)\ndev_texts = tokenize_texts(dev['text'].tolist(), tokenizer)\n\n# Convert labels to tensors\ntrain_labels = tf.convert_to_tensor(train['label'].tolist())\ndev_labels = tf.convert_to_tensor(dev['label'].tolist())\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\ntrain_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\nval_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n\n# Prepare data\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": train_texts[\"input_ids\"], \"attention_mask\": train_texts[\"attention_mask\"]},\n    train_labels\n)).batch(16)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": dev_texts[\"input_ids\"], \"attention_mask\": dev_texts[\"attention_mask\"]},\n    dev_labels\n)).batch(16)\n\n# Custom training loop\nepochs = 10\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n    \n    # Training\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n        with tf.GradientTape() as tape:\n            logits = model(x_batch_train, training=True).logits\n            loss = loss_fn(y_batch_train, logits)\n        \n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        train_acc_metric.update_state(y_batch_train, logits)\n\n    train_acc = train_acc_metric.result()\n    print(f\"Training accuracy: {float(train_acc):.4f}\")\n    train_acc_metric.reset_state()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T17:52:35.898559Z","iopub.execute_input":"2024-12-19T17:52:35.898893Z","iopub.status.idle":"2024-12-19T18:16:14.941709Z","shell.execute_reply.started":"2024-12-19T17:52:35.898864Z","shell.execute_reply":"2024-12-19T18:16:14.940836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, f1_score\n\n# Separate labels from the dev dataset\ndev_texts = dev['text'].tolist()  # Texts for prediction\ndev_labels = dev['label'].tolist()  # True labels\n\n# Tokenize dev texts\ndev_inputs = tokenize_texts(dev_texts, tokenizer)\n\n# Predict on dev texts\ndev_logits = model.predict({\"input_ids\": dev_inputs[\"input_ids\"], \n                            \"attention_mask\": dev_inputs[\"attention_mask\"]}).logits\n\n# Convert logits to predicted class labels\npredicted_labels = tf.argmax(dev_logits, axis=1).numpy()\n\n# Evaluate model performance\naccuracy = accuracy_score(dev_labels, predicted_labels)\nf1 = f1_score(dev_labels, predicted_labels, average=\"weighted\")\nreport = classification_report(dev_labels, predicted_labels, target_names=[\"Class 0\", \"Class 1\"])\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:16:26.052886Z","iopub.execute_input":"2024-12-19T18:16:26.053902Z","iopub.status.idle":"2024-12-19T18:16:33.801348Z","shell.execute_reply.started":"2024-12-19T18:16:26.053845Z","shell.execute_reply":"2024-12-19T18:16:33.800405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\nimport tensorflow as tf\n\n# Specify the model name\nmodel_name = \"l3cube-pune/malayalam-bert\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load model (with PyTorch weights if TensorFlow weights are unavailable)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, from_pt=True)\n\n# Function to tokenize texts\ndef tokenize_texts(texts, tokenizer, max_length=128):\n    return tokenizer(\n        texts,\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"tf\"\n    )\n\n# Tokenize training and validation texts\ntrain_texts = tokenize_texts(train['text'].tolist(), tokenizer)\ndev_texts = tokenize_texts(dev['text'].tolist(), tokenizer)\n\n# Convert labels to tensors\ntrain_labels = tf.convert_to_tensor(train['label'].tolist())\ndev_labels = tf.convert_to_tensor(dev['label'].tolist())\n\n# Define optimizer, loss, and metrics\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\ntrain_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\nval_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n\n# Prepare data\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": train_texts[\"input_ids\"], \"attention_mask\": train_texts[\"attention_mask\"]},\n    train_labels\n)).batch(16)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": dev_texts[\"input_ids\"], \"attention_mask\": dev_texts[\"attention_mask\"]},\n    dev_labels\n)).batch(16)\n\n# Custom training loop\nepochs = 10\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n    \n    # Training\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n        with tf.GradientTape() as tape:\n            logits = model(x_batch_train, training=True).logits\n            loss = loss_fn(y_batch_train, logits)\n        \n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        train_acc_metric.update_state(y_batch_train, logits)\n\n    # Display training accuracy\n    train_acc = train_acc_metric.result()\n    print(f\"Training accuracy: {float(train_acc):.4f}\")\n    train_acc_metric.reset_state()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:19:35.345432Z","iopub.execute_input":"2025-01-25T16:19:35.345787Z","iopub.status.idle":"2025-01-25T16:45:57.417357Z","shell.execute_reply.started":"2025-01-25T16:19:35.345755Z","shell.execute_reply":"2025-01-25T16:45:57.416313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, f1_score\n\n\n# Separate labels from the dev dataset\ndev_texts = dev['text'].tolist()  # Texts for prediction\ndev_labels = dev['label'].tolist()  # True labels\n\n# Tokenize dev texts\ndev_inputs = tokenize_texts(dev_texts, tokenizer)\n\n# Predict on dev texts\ndev_logits = model.predict({\"input_ids\": dev_inputs[\"input_ids\"], \n                            \"attention_mask\": dev_inputs[\"attention_mask\"]}).logits\n\n# Convert logits to predicted class labels\npredicted_labels = tf.argmax(dev_logits, axis=1).numpy()\n\n# Evaluate model performance\naccuracy = accuracy_score(dev_labels, predicted_labels)\nf1 = f1_score(dev_labels, predicted_labels, average=\"weighted\")\nreport = classification_report(dev_labels, predicted_labels, target_names=[\"Class 0\", \"Class 1\"])\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:46:42.607289Z","iopub.execute_input":"2025-01-25T16:46:42.608101Z","iopub.status.idle":"2025-01-25T16:47:05.910978Z","shell.execute_reply.started":"2025-01-25T16:46:42.608064Z","shell.execute_reply":"2025-01-25T16:47:05.909868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\n\n# Step 1: Get the test texts\ntest_texts = test['text'].tolist()\ntest_inputs = tokenize_texts(test_texts, tokenizer)\n\n# Step 2: Predict labels for test data\ntest_logits = model.predict({\n    \"input_ids\": test_inputs[\"input_ids\"], \n    \"attention_mask\": test_inputs[\"attention_mask\"]\n}).logits\n\n# Convert logits to predicted class labels (numerical)\ntest['label'] = tf.argmax(test_logits, axis=1).numpy()\n\n# Step 3: Map numerical labels back to \"fake\" and \"original\"\nlabel_mapping = {0: \"Fake\", 1: \"original\"}\ntest['label'] = test['label'].map(label_mapping)\n\n# Step 4: Create submission CSV files for multiple runs\nteam_name = \"One_by_zero\"\nsubmission_dir = f\"{team_name}\"\nos.makedirs(submission_dir, exist_ok=True)\n\nsubmission_file = f\"{submission_dir}/{team_name}_Malayalam_Task1_run1.csv\"\ntest[['Id', 'label']].rename(columns={'label': 'Labels'}).to_csv(submission_file, index=False)\n\n# Step 5: Zip the submission files\nzip_filename = f\"{team_name}.zip\"\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    submission_file = f\"{submission_dir}/{team_name}_Malayalam_Task1_run1.csv\"\n    zipf.write(submission_file)\n\nprint(f\"Submission file {zip_filename} created successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:47:19.573838Z","iopub.execute_input":"2025-01-25T16:47:19.574459Z","iopub.status.idle":"2025-01-25T16:47:29.662394Z","shell.execute_reply.started":"2025-01-25T16:47:19.574423Z","shell.execute_reply":"2025-01-25T16:47:29.661576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the test labels (ground truth)\ntest_labels = test_label['label'].tolist()  # The true labels\n\n# Get the predicted labels from your model (from 'test')\npredicted_labels = test['label'].tolist()  # The predicted labels\n\n# Generate the confusion matrix\ncm = confusion_matrix(test_labels, predicted_labels, labels=[\"Fake\", \"original\"])\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Fake\", \"original\"], yticklabels=[\"Fake\", \"original\"])\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Optionally, print the confusion matrix as well\nprint(\"Confusion Matrix:\")\nprint(cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:47:35.069428Z","iopub.execute_input":"2025-01-25T16:47:35.069768Z","iopub.status.idle":"2025-01-25T16:47:35.604549Z","shell.execute_reply.started":"2025-01-25T16:47:35.069738Z","shell.execute_reply":"2025-01-25T16:47:35.603671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nimport pandas as pd\n\n# Step 1: Ensure the correct labels and predicted labels are available\ntest['correct_label'] = test_label['label']  # Add the true labels to the test DataFrame\n\n# Step 2: Evaluate the model's performance\naccuracy = accuracy_score(test['correct_label'], test['label'])\nprecision = precision_score(test['correct_label'], test['label'], average='weighted')\nrecall = recall_score(test['correct_label'], test['label'], average='weighted')\nf1 = f1_score(test['correct_label'], test['label'], average='weighted')\nreport = classification_report(test['correct_label'], test['label'], target_names=[\"Fake\", \"original\"])\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(\"\\nClassification Report:\\n\", report)\n\n# Step 3: Create a comparison DataFrame for the first 5 rows\ncomparison_df = test[['text', 'correct_label', 'label']].head(5).rename(\n    columns={'text': 'News', 'correct_label': 'True Label', 'label': 'Predicted Label'}\n)\n\n# Step 4: Display the comparison\nprint(\"\\nComparison of the first 5 rows:\")\nprint(comparison_df)\n\n# Optionally, you can save the comparison DataFrame for reference\ncomparison_df.to_csv('comparison_first_5.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:47:50.716674Z","iopub.execute_input":"2025-01-25T16:47:50.717764Z","iopub.status.idle":"2025-01-25T16:47:50.801116Z","shell.execute_reply.started":"2025-01-25T16:47:50.717726Z","shell.execute_reply":"2025-01-25T16:47:50.800177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\nimport tensorflow as tf\n\n# Load the tokenizer and model for IndicBERT\ntokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBERTv2-MLM-only\")\n\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\n    \"ai4bharat/IndicBERTv2-MLM-only\", \n    num_labels=2,from_pt=True  # Adjust the number of labels as per your classification task\n)\n\n# Function to tokenize texts\ndef tokenize_texts(texts, tokenizer, max_length=60):\n    return tokenizer(\n        texts,\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"tf\"\n    )\n\n# Tokenize training and validation texts\ntrain_texts = tokenize_texts(train['text'].tolist(), tokenizer)\ndev_texts = tokenize_texts(dev['text'].tolist(), tokenizer)\n\n# Convert labels to tensors\ntrain_labels = tf.convert_to_tensor(train['label'].tolist())\ndev_labels = tf.convert_to_tensor(dev['label'].tolist())\n\n# Define optimizer, loss, and metrics\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\ntrain_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\nval_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n\n# Prepare data\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": train_texts[\"input_ids\"], \"attention_mask\": train_texts[\"attention_mask\"]},\n    train_labels\n)).batch(16)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": dev_texts[\"input_ids\"], \"attention_mask\": dev_texts[\"attention_mask\"]},\n    dev_labels\n)).batch(16)\n\n# Custom training loop\nepochs = 10\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n    \n    # Training\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n        with tf.GradientTape() as tape:\n            logits = model(x_batch_train, training=True).logits\n            loss = loss_fn(y_batch_train, logits)\n        \n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        train_acc_metric.update_state(y_batch_train, logits)\n\n    # Display training accuracy\n    train_acc = train_acc_metric.result()\n    print(f\"Training accuracy: {float(train_acc):.4f}\")\n    train_acc_metric.reset_state()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:42:21.380347Z","iopub.execute_input":"2025-01-26T06:42:21.381154Z","iopub.status.idle":"2025-01-26T07:04:59.975353Z","shell.execute_reply.started":"2025-01-26T06:42:21.381121Z","shell.execute_reply":"2025-01-26T07:04:59.974327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, f1_score\n\n# Separate labels from the dev dataset\ndev_texts = dev['text'].tolist()  # Texts for prediction\ndev_labels = dev['label'].tolist()  # True labels\n\n# Tokenize dev texts\ndev_inputs = tokenize_texts(dev_texts, tokenizer)\n\n# Predict on dev texts\ndev_logits = model.predict({\"input_ids\": dev_inputs[\"input_ids\"], \n                            \"attention_mask\": dev_inputs[\"attention_mask\"]}).logits\n\n# Convert logits to predicted class labels\npredicted_labels = tf.argmax(dev_logits, axis=1).numpy()\n\n# Evaluate model performance\naccuracy = accuracy_score(dev_labels, predicted_labels)\nf1 = f1_score(dev_labels, predicted_labels, average=\"weighted\")\nreport = classification_report(dev_labels, predicted_labels, target_names=[\"Class 0\", \"Class 1\"])\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T07:15:32.067200Z","iopub.execute_input":"2025-01-26T07:15:32.067569Z","iopub.status.idle":"2025-01-26T07:15:45.266192Z","shell.execute_reply.started":"2025-01-26T07:15:32.067541Z","shell.execute_reply":"2025-01-26T07:15:45.265350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nimport pandas as pd\n\n# Step 1: Ensure the correct labels and predicted labels are available\ntest['correct_label'] = test_label['label']  # Add the true labels to the test DataFrame\n\n# Step 2: Evaluate the model's performance\naccuracy = accuracy_score(test['correct_label'], test['label'])\nprecision = precision_score(test['correct_label'], test['label'], average='weighted')\nrecall = recall_score(test['correct_label'], test['label'], average='weighted')\nf1 = f1_score(test['correct_label'], test['label'], average='weighted')\nreport = classification_report(test['correct_label'], test['label'], target_names=[\"Fake\", \"original\"])\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(\"\\nClassification Report:\\n\", report)\n\n# Step 3: Create a comparison DataFrame for the first 5 rows\ncomparison_df = test[['text', 'correct_label', 'label']].head(5).rename(\n    columns={'text': 'News', 'correct_label': 'True Label', 'label': 'Predicted Label'}\n)\n\n# Step 4: Display the comparison\nprint(\"\\nComparison of the first 5 rows:\")\nprint(comparison_df)\n\n# Optionally, you can save the comparison DataFrame for reference\ncomparison_df.to_csv('comparison_first_5.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T07:16:05.122718Z","iopub.execute_input":"2025-01-26T07:16:05.123207Z","iopub.status.idle":"2025-01-26T07:16:05.195526Z","shell.execute_reply.started":"2025-01-26T07:16:05.123173Z","shell.execute_reply":"2025-01-26T07:16:05.194804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\n\n# Step 1: Get the test texts\ntest_texts = test['text'].tolist()\ntest_inputs = tokenize_texts(test_texts, tokenizer)\n\n# Step 2: Predict labels for test data\ntest_logits = model.predict({\n    \"input_ids\": test_inputs[\"input_ids\"], \n    \"attention_mask\": test_inputs[\"attention_mask\"]\n}).logits\n\n# Convert logits to predicted class labels (numerical)\ntest['label'] = tf.argmax(test_logits, axis=1).numpy()\n\n# Step 3: Map numerical labels back to \"fake\" and \"original\"\nlabel_mapping = {0: \"Fake\", 1: \"original\"}\ntest['label'] = test['label'].map(label_mapping)\n\n# Step 4: Create submission CSV files for multiple runs\nteam_name = \"One_by_zero\"\nsubmission_dir = f\"{team_name}\"\nos.makedirs(submission_dir, exist_ok=True)\n\nsubmission_file = f\"{submission_dir}/{team_name}_Malayalam_Task1_run2.csv\"\ntest[['Id', 'label']].rename(columns={'label': 'Labels'}).to_csv(submission_file, index=False)\n\n# Step 5: Zip the submission files\nzip_filename = f\"{team_name}.zip\"\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    submission_file = f\"{submission_dir}/{team_name}_Malayalam_Task1_run2.csv\"\n    zipf.write(submission_file)\n\nprint(f\"Submission file {zip_filename} created successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T07:15:57.716175Z","iopub.execute_input":"2025-01-26T07:15:57.717001Z","iopub.status.idle":"2025-01-26T07:16:02.205242Z","shell.execute_reply.started":"2025-01-26T07:15:57.716942Z","shell.execute_reply":"2025-01-26T07:16:02.204398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\nimport tensorflow as tf\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\n\n# Load the tokenizer and model for FakeNews-Classifier-GPT-Labels\ntokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\n    \"FacebookAI/xlm-roberta-base\", \n    num_labels=2,  # Adjust if your task involves more than two labels\n    from_pt=True\n)\n\n\n  \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:02:29.401285Z","iopub.execute_input":"2025-01-26T06:02:29.402009Z","iopub.status.idle":"2025-01-26T06:02:42.939278Z","shell.execute_reply.started":"2025-01-26T06:02:29.401949Z","shell.execute_reply":"2025-01-26T06:02:42.938549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_texts(texts, tokenizer, max_length=60):\n    return tokenizer(\n        texts,\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"tf\"\n    )\n\ntrain_texts = tokenize_texts(train['text'].tolist(), tokenizer)\ndev_texts = tokenize_texts(dev['text'].tolist(), tokenizer)\n\n# Convert labels to tensors\ntrain_labels = tf.convert_to_tensor(train['label'].tolist())\ndev_labels = tf.convert_to_tensor(dev['label'].tolist())\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\ntrain_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\nval_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n\n# Prepare data\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": train_texts[\"input_ids\"], \"attention_mask\": train_texts[\"attention_mask\"]},\n    train_labels\n)).batch(16)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": dev_texts[\"input_ids\"], \"attention_mask\": dev_texts[\"attention_mask\"]},\n    dev_labels\n)).batch(16)\n\n# Custom training loop\nepochs = 15\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n    \n    # Training\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n        with tf.GradientTape() as tape:\n            logits = model(x_batch_train, training=True).logits\n            loss = loss_fn(y_batch_train, logits)\n        \n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        train_acc_metric.update_state(y_batch_train, logits)\n\n    train_acc = train_acc_metric.result()\n    print(f\"Training accuracy: {float(train_acc):.4f}\")\n    train_acc_metric.reset_state()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:02:47.707536Z","iopub.execute_input":"2025-01-26T06:02:47.707883Z","iopub.status.idle":"2025-01-26T06:36:23.711635Z","shell.execute_reply.started":"2025-01-26T06:02:47.707846Z","shell.execute_reply":"2025-01-26T06:36:23.710688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, f1_score\n\n# Separate labels from the dev dataset\ndev_texts = dev['text'].tolist()  # Texts for prediction\ndev_labels = dev['label'].tolist()  # True labels\n\n# Tokenize dev texts\ndev_inputs = tokenize_texts(dev_texts, tokenizer)\n\n# Predict on dev texts\ndev_logits = model.predict({\"input_ids\": dev_inputs[\"input_ids\"], \n                            \"attention_mask\": dev_inputs[\"attention_mask\"]}).logits\n\n# Convert logits to predicted class labels\npredicted_labels = tf.argmax(dev_logits, axis=1).numpy()\n\n# Evaluate model performance\naccuracy = accuracy_score(dev_labels, predicted_labels)\nf1 = f1_score(dev_labels, predicted_labels, average=\"weighted\")\nreport = classification_report(dev_labels, predicted_labels, target_names=[\"Class 0\", \"Class 1\"])\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:36:45.406795Z","iopub.execute_input":"2025-01-26T06:36:45.407171Z","iopub.status.idle":"2025-01-26T06:37:04.223361Z","shell.execute_reply.started":"2025-01-26T06:36:45.407140Z","shell.execute_reply":"2025-01-26T06:37:04.222391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\n\n# Step 1: Get the test texts\ntest_texts = test['text'].tolist()\ntest_inputs = tokenize_texts(test_texts, tokenizer)\n\n# Step 2: Predict labels for test data\ntest_logits = model.predict({\n    \"input_ids\": test_inputs[\"input_ids\"], \n    \"attention_mask\": test_inputs[\"attention_mask\"]\n}).logits\n\n# Convert logits to predicted class labels (numerical)\ntest['label'] = tf.argmax(test_logits, axis=1).numpy()\n\n# Step 3: Map numerical labels back to \"fake\" and \"original\"\nlabel_mapping = {0: \"Fake\", 1: \"original\"}\ntest['label'] = test['label'].map(label_mapping)\n\n# Step 4: Create submission CSV files for multiple runs\nteam_name = \"One_by_zero\"\nsubmission_dir = f\"{team_name}\"\nos.makedirs(submission_dir, exist_ok=True)\n\nsubmission_file = f\"{submission_dir}/{team_name}_Malayalam_Task1_run2.csv\"\ntest[['Id', 'label']].rename(columns={'label': 'Labels'}).to_csv(submission_file, index=False)\n\n# Step 5: Zip the submission files\nzip_filename = f\"{team_name}.zip\"\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    submission_file = f\"{submission_dir}/{team_name}_Malayalam_Task1_run2.csv\"\n    zipf.write(submission_file)\n\nprint(f\"Submission file {zip_filename} created successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:37:23.712767Z","iopub.execute_input":"2025-01-26T06:37:23.713137Z","iopub.status.idle":"2025-01-26T06:37:28.326832Z","shell.execute_reply.started":"2025-01-26T06:37:23.713100Z","shell.execute_reply":"2025-01-26T06:37:28.326016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nimport pandas as pd\n\n# Step 1: Ensure the correct labels and predicted labels are available\ntest['correct_label'] = test_label['label']  # Add the true labels to the test DataFrame\n\n# Step 2: Evaluate the model's performance\naccuracy = accuracy_score(test['correct_label'], test['label'])\nprecision = precision_score(test['correct_label'], test['label'], average='weighted')\nrecall = recall_score(test['correct_label'], test['label'], average='weighted')\nf1 = f1_score(test['correct_label'], test['label'], average='weighted')\nreport = classification_report(test['correct_label'], test['label'], target_names=[\"Fake\", \"original\"])\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(\"\\nClassification Report:\\n\", report)\n\n# Step 3: Create a comparison DataFrame for the first 5 rows\ncomparison_df = test[['text', 'correct_label', 'label']].head(5).rename(\n    columns={'text': 'News', 'correct_label': 'True Label', 'label': 'Predicted Label'}\n)\n\n# Step 4: Display the comparison\nprint(\"\\nComparison of the first 5 rows:\")\nprint(comparison_df)\n\n# Optionally, you can save the comparison DataFrame for reference\ncomparison_df.to_csv('comparison_first_5.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:37:32.927404Z","iopub.execute_input":"2025-01-26T06:37:32.927740Z","iopub.status.idle":"2025-01-26T06:37:33.002393Z","shell.execute_reply.started":"2025-01-26T06:37:32.927712Z","shell.execute_reply":"2025-01-26T06:37:33.001539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"l3cube-pune/malayalam-sentence-bert-nli\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"l3cube-pune/malayalam-sentence-bert-nli\",\n    num_labels=2  # Adjust based on your task\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T08:18:49.237075Z","iopub.execute_input":"2025-01-26T08:18:49.237844Z","iopub.status.idle":"2025-01-26T08:18:55.610836Z","shell.execute_reply.started":"2025-01-26T08:18:49.237811Z","shell.execute_reply":"2025-01-26T08:18:55.610202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\nimport torch # Ensure you use AdamW from transformers\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import AdamW\ndef tokenize_texts(texts, tokenizer, max_length=60):\n    return tokenizer(\n        texts,\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\"  # Change to PyTorch tensors\n    )\n\ntrain_texts = tokenize_texts(train['text'].tolist(), tokenizer)\ndev_texts = tokenize_texts(dev['text'].tolist(), tokenizer)\n\n# Convert labels to tensors (assuming they are already numeric)\ntrain_labels = torch.tensor(train['label'].tolist())\ndev_labels = torch.tensor(dev['label'].tolist())\n\n# Create DataLoader for batching\ntrain_dataset = TensorDataset(train_texts[\"input_ids\"], train_texts[\"attention_mask\"], train_labels)\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\ndev_dataset = TensorDataset(dev_texts[\"input_ids\"], dev_texts[\"attention_mask\"], dev_labels)\ndev_dataloader = DataLoader(dev_dataset, batch_size=16, shuffle=False)\n\n# Define optimizer, loss function, and device\noptimizer = AdamW(model.parameters(), lr=2e-5)\nloss_fn = CrossEntropyLoss()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Custom training loop\nepochs = 15\nfor epoch in range(epochs):\n    model.train()  # Set model to training mode\n    total_train_loss = 0\n    total_train_correct = 0\n    total_train_samples = 0\n\n    for batch in train_dataloader:\n        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n        \n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        \n        # Compute loss and backpropagate\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        # Track training accuracy\n        preds = torch.argmax(logits, dim=1)\n        total_train_correct += (preds == labels).sum().item()\n        total_train_samples += labels.size(0)\n        total_train_loss += loss.item()\n\n    train_accuracy = total_train_correct / total_train_samples\n    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {total_train_loss / len(train_dataloader):.4f}, Train Accuracy: {train_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T08:20:00.547504Z","iopub.execute_input":"2025-01-26T08:20:00.548382Z","iopub.status.idle":"2025-01-26T08:31:48.415512Z","shell.execute_reply.started":"2025-01-26T08:20:00.548332Z","shell.execute_reply":"2025-01-26T08:31:48.414537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, f1_score\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Separate texts and labels from the dev dataset\ndev_texts_list = dev['text'].tolist()  # Texts for prediction\ndev_labels_list = dev['label'].tolist()  # True labels\n\n# Tokenize dev texts\ndev_inputs = tokenize_texts(dev_texts_list, tokenizer)\n\n# Move inputs to the same device as the model\ninput_ids = dev_inputs[\"input_ids\"].to(device)\nattention_mask = dev_inputs[\"attention_mask\"].to(device)\n\n# Perform inference without gradient computation\nwith torch.no_grad():\n    outputs = model(input_ids, attention_mask=attention_mask)\n    logits = outputs.logits\n\n# Convert logits to predicted class labels\npredicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n\n# Evaluate model performance\naccuracy = accuracy_score(dev_labels_list, predicted_labels)\nf1 = f1_score(dev_labels_list, predicted_labels, average=\"weighted\")\nreport = classification_report(dev_labels_list, predicted_labels, target_names=[\"Class 0\", \"Class 1\"])\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T08:31:48.417516Z","iopub.execute_input":"2025-01-26T08:31:48.418161Z","iopub.status.idle":"2025-01-26T08:31:51.392853Z","shell.execute_reply.started":"2025-01-26T08:31:48.418118Z","shell.execute_reply":"2025-01-26T08:31:51.391977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\nfrom torch.utils.data import DataLoader\n\n# Step 1: Ensure model is in evaluation mode\nmodel.eval()\n\n# Step 2: Predict labels for test data\ntest_texts = test['text'].tolist()\ntest_inputs = tokenize_texts(test_texts, tokenizer)\n\n# Move test inputs to the same device as the model\ninput_ids = test_inputs[\"input_ids\"].to(device)\nattention_mask = test_inputs[\"attention_mask\"].to(device)\n\n# Perform inference without gradient computation\nwith torch.no_grad():\n    outputs = model(input_ids, attention_mask=attention_mask)\n    test_logits = outputs.logits\n\n# Convert logits to predicted class labels\npredicted_labels = torch.argmax(test_logits, dim=1).cpu().numpy()\n\n# Map numerical labels back to string labels\nlabel_mapping = {0: \"Fake\", 1: \"original\"}\ntest['label'] = [label_mapping[label] for label in predicted_labels]\n\n# Step 3: Create submission CSV file\nteam_name = \"One_by_zero\"\nsubmission_dir = f\"{team_name}\"\nos.makedirs(submission_dir, exist_ok=True)\n\nsubmission_file = f\"{submission_dir}/{team_name}_Malayalam_Task1_run3.csv\"\ntest[['Id', 'label']].rename(columns={'label': 'Labels'}).to_csv(submission_file, index=False)\n\n# Step 4: Zip the submission files\nzip_filename = f\"{team_name}.zip\"\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    submission_file = f\"{submission_dir}/{team_name}_Malayalam_Task1_run3.csv\"\n    zipf.write(submission_file)\n\nprint(f\"Submission file {zip_filename} created successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T08:32:11.845992Z","iopub.execute_input":"2025-01-26T08:32:11.846643Z","iopub.status.idle":"2025-01-26T08:32:15.200168Z","shell.execute_reply.started":"2025-01-26T08:32:11.846611Z","shell.execute_reply":"2025-01-26T08:32:15.199085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nimport pandas as pd\n\n# Step 1: Ensure the correct labels and predicted labels are available\ntest['correct_label'] = test_label['label']  # Add the true labels to the test DataFrame\n\n# Step 2: Evaluate the model's performance\naccuracy = accuracy_score(test['correct_label'], test['label'])\nprecision = precision_score(test['correct_label'], test['label'], average='weighted')\nrecall = recall_score(test['correct_label'], test['label'], average='weighted')\nf1 = f1_score(test['correct_label'], test['label'], average='weighted')\nreport = classification_report(test['correct_label'], test['label'], target_names=[\"Fake\", \"original\"])\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(\"\\nClassification Report:\\n\", report)\n\n# Step 3: Create a comparison DataFrame for the first 5 rows\ncomparison_df = test[['text', 'correct_label', 'label']].head(5).rename(\n    columns={'text': 'News', 'correct_label': 'True Label', 'label': 'Predicted Label'}\n)\n\n# Step 4: Display the comparison\nprint(\"\\nComparison of the first 5 rows:\")\nprint(comparison_df)\n\n# Optionally, you can save the comparison DataFrame for reference\ncomparison_df.to_csv('comparison_first_5.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T08:32:18.497890Z","iopub.execute_input":"2025-01-26T08:32:18.498730Z","iopub.status.idle":"2025-01-26T08:32:18.575556Z","shell.execute_reply.started":"2025-01-26T08:32:18.498695Z","shell.execute_reply":"2025-01-26T08:32:18.574799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}